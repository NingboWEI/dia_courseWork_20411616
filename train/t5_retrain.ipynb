{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a75e93c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nibaaa\\.conda\\envs\\dia\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['File_path', 'Articles', 'Summaries'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import unicodedata\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def clean_text_for_inference(text):\n",
    "    def unicode_to_ascii(s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "    text = unicode_to_ascii(text.lower().strip())\n",
    "    # convert ... and .. to <title_end> and <p>\n",
    "    # however, we can not use \"<>\" since it will be removed by tokenizer in default\n",
    "\n",
    "    text = re.sub(r\"\\.\\.\\.\", \" \", text)\n",
    "    text = re.sub(r\".*?\\.\\.\", \"\", text)\n",
    "    \n",
    "    # Abbreviation Restoration & Stem Preservation\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "\n",
    "    # Multiple spaces merge\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "loadDataSize = 500\n",
    "df = pd.read_csv(\"../data/bbc-news-summary.csv\").dropna()\n",
    "print(df.columns)\n",
    "per_class_count = int(loadDataSize / 5)\n",
    "selected_dfs = []\n",
    "\n",
    "for category in df['File_path'].unique():\n",
    "\tcategory_df = df[df['File_path'] == category].head(per_class_count)\n",
    "\tselected_dfs.append(category_df)\n",
    "\n",
    "selected_df = pd.concat(selected_dfs).sample(frac=1).reset_index(drop=True)  # shuffle the selected data\n",
    "\n",
    "# 清理文本并添加 <sos> 和 <eos>\n",
    "selected_df['input_text'] = selected_df['Articles'].apply(clean_text_for_inference)\n",
    "selected_df['target_text'] = selected_df['Summaries'].apply(clean_text_for_inference)\n",
    "\n",
    "\n",
    "dataset = Dataset.from_pandas(selected_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79b95fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_path</th>\n",
       "      <th>Articles</th>\n",
       "      <th>Summaries</th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>politics</td>\n",
       "      <td>Kilroy names election seat target..Ex-chat sho...</td>\n",
       "      <td>UKIP's leader, Roger Knapman, has said he is g...</td>\n",
       "      <td>ex-chat show host robert kilroy-silk is to con...</td>\n",
       "      <td>ukip's leader, roger knapman, has said he is g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>Mitsubishi in Peugeot link talks..Trouble-hit ...</td>\n",
       "      <td>Trouble-hit Mitsubishi Motors is in talks with...</td>\n",
       "      <td>trouble-hit mitsubishi motors is in talks with...</td>\n",
       "      <td>trouble-hit mitsubishi motors is in talks with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>Court halts Mark Morrison album..Premiership f...</td>\n",
       "      <td>Premiership footballer and record company boss...</td>\n",
       "      <td>but morrison is determined the album will be r...</td>\n",
       "      <td>premiership footballer and record company boss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>US 'to raise TV indecency fines'..US politicia...</td>\n",
       "      <td>Last year's Janet Jackson 'wardrobe malfunctio...</td>\n",
       "      <td>us politicians are proposing a tough new law a...</td>\n",
       "      <td>last year's janet jackson 'wardrobe malfunctio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>Mild winter drives US oil down 6%..US oil pric...</td>\n",
       "      <td>US oil prices have fallen by 6%, driven down b...</td>\n",
       "      <td>us oil prices have fallen by 6%, driven down b...</td>\n",
       "      <td>us oil prices have fallen by 6%, driven down b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       File_path                                           Articles  \\\n",
       "0       politics  Kilroy names election seat target..Ex-chat sho...   \n",
       "1       business  Mitsubishi in Peugeot link talks..Trouble-hit ...   \n",
       "2  entertainment  Court halts Mark Morrison album..Premiership f...   \n",
       "3  entertainment  US 'to raise TV indecency fines'..US politicia...   \n",
       "4       business  Mild winter drives US oil down 6%..US oil pric...   \n",
       "\n",
       "                                           Summaries  \\\n",
       "0  UKIP's leader, Roger Knapman, has said he is g...   \n",
       "1  Trouble-hit Mitsubishi Motors is in talks with...   \n",
       "2  Premiership footballer and record company boss...   \n",
       "3  Last year's Janet Jackson 'wardrobe malfunctio...   \n",
       "4  US oil prices have fallen by 6%, driven down b...   \n",
       "\n",
       "                                          input_text  \\\n",
       "0  ex-chat show host robert kilroy-silk is to con...   \n",
       "1  trouble-hit mitsubishi motors is in talks with...   \n",
       "2  but morrison is determined the album will be r...   \n",
       "3  us politicians are proposing a tough new law a...   \n",
       "4  us oil prices have fallen by 6%, driven down b...   \n",
       "\n",
       "                                         target_text  \n",
       "0  ukip's leader, roger knapman, has said he is g...  \n",
       "1  trouble-hit mitsubishi motors is in talks with...  \n",
       "2  premiership footballer and record company boss...  \n",
       "3  last year's janet jackson 'wardrobe malfunctio...  \n",
       "4  us oil prices have fallen by 6%, driven down b...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6a8e05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ex-chat show host robert kilroy-silk is to contest the derbyshire seat of erewash at the next general election labour's elizabeth blackman won the seat in 1997 and has a 6,932 majority. she says she will fight on her record \"as a hard-working constituency mp\". mr kilroy-silk announced his plans a day after launching his new party, veritas, the latin for truth. the east midlands mep, who quit the uk independence party, wants his new group to \"change the face\" of uk politics. his choice of election constituency quashes speculation that he would stand against defence secretary geoff hoon in ashfield, nottinghamshire. ukip won 31% of the vote in erewash in last june's european elections - with mr kilroy-silk among their candidates for the region. until 1997, erewash had been held by the tories since 1970. ms blackman said she was proud of the government's achievements in the area. she declined to give her view of mr kilroy-silk at this point on thursday, he told a london news conference that veritas would avoid the old parties' \"lies and spin\". he said \"our country\" was being \"stolen from us\" by mass immigration and promised a \"firm but fair\" policy on immigration. veritas says it hopes to contest most seats at the forthcoming general election but plans to announce detailed policies on crime, tax, pensions, health and defence over the next few weeks ukip leader roger knapman says he is glad to see the back of mr kilroy-silk. labour campaign spokesman fraser kemp said veritas was joining \"an already crowded field on the right of british politics\". mr kilroy-silk was joined in the new venture by one of ukip's two london assembly members, damien hockney, who is now veritas' deputy leader ukip's chairman petrina holdsworth has said the group will just be a parody of the party the men have left. mr kilroy-silk quit ukip last week after months of tension as he vied unsuccessfully for the leadership of that party. he said he was ashamed to be a member of a ukip whose leadership had \"gone awol\" after the great opportunity offered by its third place at last june's european elections. ukip's leader, roger knapman, has said he is glad to see the back of mr kilroy-silk. \"he has remarkable ability to influence people but, sadly, after the [european] election it became clear that he was more interested in the robert kilroy-silk party than the uk independence party so it was nice knowing him, now 'goodbye',\" he said. ukip officials also argue mr kilroy-silk has not been \"straightforward\" in attacking the party he once wanted to lead.\n",
      "ukip's leader, roger knapman, has said he is glad to see the back of mr kilroy-silk.\"he has remarkable ability to influence people but, sadly, after the [european] election it became clear that he was more interested in the robert kilroy-silk party than the uk independence party so it was nice knowing him, now 'goodbye',\" he said.ukip leader roger knapman says he is glad to see the back of mr kilroy-silk.ukip won 31% of the vote in erewash in last june's european elections - with mr kilroy-silk among their candidates for the region.ukip officials also argue mr kilroy-silk has not been \"straightforward\" in attacking the party he once wanted to lead.mr kilroy-silk announced his plans a day after launching his new party, veritas, the latin for truth.mr kilroy-silk quit ukip last week after months of tension as he vied unsuccessfully for the leadership of that party.mr kilroy-silk was joined in the new venture by one of ukip's two london assembly members, damien hockney, who is now veritas' deputy leader.ex-chat show host robert kilroy-silk is to contest the derbyshire seat of erewash at the next general election.\n"
     ]
    }
   ],
   "source": [
    "print(selected_df.iloc[0][\"input_text\"])\n",
    "print(selected_df.iloc[0][\"target_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c9083d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# 加载模型和分词器\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# 设置设备（使用 GPU 加速）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a831a95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/500 [00:00<?, ? examples/s]c:\\Users\\nibaaa\\.conda\\envs\\dia\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 565.13 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [\"summarize: \" + doc for doc in examples[\"input_text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"target_text\"], max_length=160, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# 对数据集进行预处理\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "059f4465",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",   # 确保评估和保存策略一致\n",
    "    save_strategy=\"epoch\",         # 设置为 steps\n",
    "\tlogging_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=12,\n",
    "    per_device_eval_batch_size=12,\n",
    "    num_train_epochs=45,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69150279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nibaaa\\AppData\\Local\\Temp\\ipykernel_47740\\261569621.py:5: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1890' max='1890' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1890/1890 11:45, Epoch 45/45]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.784433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.433337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.229100</td>\n",
       "      <td>1.258903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.229100</td>\n",
       "      <td>1.168095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.400700</td>\n",
       "      <td>1.117730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.400700</td>\n",
       "      <td>1.090694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.400700</td>\n",
       "      <td>1.064648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.262100</td>\n",
       "      <td>1.047797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.262100</td>\n",
       "      <td>1.031112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.200700</td>\n",
       "      <td>1.017734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.200700</td>\n",
       "      <td>1.007977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.149300</td>\n",
       "      <td>0.996801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.149300</td>\n",
       "      <td>0.988418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.149300</td>\n",
       "      <td>0.977776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.126500</td>\n",
       "      <td>0.971053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.126500</td>\n",
       "      <td>0.965617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.112200</td>\n",
       "      <td>0.958409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.112200</td>\n",
       "      <td>0.951991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.112200</td>\n",
       "      <td>0.945890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.094100</td>\n",
       "      <td>0.939427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.094100</td>\n",
       "      <td>0.935845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.069500</td>\n",
       "      <td>0.932590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.069500</td>\n",
       "      <td>0.926434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.067700</td>\n",
       "      <td>0.923218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.067700</td>\n",
       "      <td>0.919056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.067700</td>\n",
       "      <td>0.915466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.059700</td>\n",
       "      <td>0.912262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.059700</td>\n",
       "      <td>0.909702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.032100</td>\n",
       "      <td>0.906941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.032100</td>\n",
       "      <td>0.905032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.054300</td>\n",
       "      <td>0.901799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.054300</td>\n",
       "      <td>0.899653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.054300</td>\n",
       "      <td>0.897676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.005600</td>\n",
       "      <td>0.895592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.005600</td>\n",
       "      <td>0.893430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.063500</td>\n",
       "      <td>0.892315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.063500</td>\n",
       "      <td>0.891111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.063500</td>\n",
       "      <td>0.889883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.031400</td>\n",
       "      <td>0.888838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.031400</td>\n",
       "      <td>0.888157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.033300</td>\n",
       "      <td>0.887395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.033300</td>\n",
       "      <td>0.887155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.007400</td>\n",
       "      <td>0.886795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.007400</td>\n",
       "      <td>0.886470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.007400</td>\n",
       "      <td>0.886407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1890, training_loss=1.1596119552693038, metrics={'train_runtime': 705.8576, 'train_samples_per_second': 31.876, 'train_steps_per_second': 2.678, 'total_flos': 3041905322360832.0, 'train_loss': 1.1596119552693038, 'epoch': 45.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f61f80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fine-tuned-t5-small-news-summarization\\\\tokenizer_config.json',\n",
       " 'fine-tuned-t5-small-news-summarization\\\\special_tokens_map.json',\n",
       " 'fine-tuned-t5-small-news-summarization\\\\spiece.model',\n",
       " 'fine-tuned-t5-small-news-summarization\\\\added_tokens.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存微调后的模型\n",
    "model.save_pretrained(\"fine-tuned-t5-small-news-summarization\")\n",
    "tokenizer.save_pretrained(\"fine-tuned-t5-small-news-summarization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b617b3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': \"p A senior source earlier this afternoon told the BBC that Ukraine was ready to sign the deal today and that economy minister Yulia Svyrdenko was en route to Washington.p Ukraine's Prime Minister Denys Shmyhal then said that the fine details were being worked on and he hoped it would be signed in the next 24 hours.p In Washington, the first we heard from the administration was at the end of a cabinet meeting marking\"}]\n"
     ]
    }
   ],
   "source": [
    "# 加载微调后的模型\n",
    "from transformers import pipeline\n",
    "\n",
    "test_input = \"\"\"US ready to sign Ukraine minerals deal ‘this afternoon’, as Kyiv sends minister to Washington title_end The latest line from US Treasury Secretary Scott Bessent is that the US is ready to sign the deal if Ukraine is - but let's take a moment to look back at today's developments. p A senior source earlier this afternoon told the BBC that Ukraine was ready to sign the deal today and that economy minister Yulia Svyrdenko was en route to Washington. p Ukraine's Prime Minister Denys Shmyhal then said that the fine details were being worked on and he hoped it would be signed in the next 24 hours. p In Washington, the first we heard from the administration was at the end of a cabinet meeting marking the first 100 days of Trump's second term. Bessent responded to a question from the press and indicated that the US was ready to finalise the agreement after some \"last minute changes\".\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"fine-tuned-t5-small-news-summarization\")\n",
    "\n",
    "summary = summarizer(test_input, max_length=100, min_length=30, do_sample=False)\n",
    "print(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
