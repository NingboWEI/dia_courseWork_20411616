{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import sys\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(os.path.abspath(os.path.join(current_dir, '..')))\n",
    "from tokenizerManager import TokenizerManager\n",
    "from data.utils_bbc import get_dataloaders as get_dataloaders_bbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOADDATASIZE = 1500  # row of data\n",
    "BATCH_SIZE = 28  # batch size\n",
    "PAD_TOKEN_ID = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check GPU avaiabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ctext</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>India will host its first WTA tournament in fi...</td>\n",
       "      <td>India will host its first WTA tournament in fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Samajwadi Party vice-president Kiranmay Nanda,...</td>\n",
       "      <td>Ousted SP Vice President Kiranmoy Nanda, who w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gurgaon, Jul 7 (PTI) Gurgaon Police today regi...</td>\n",
       "      <td>Servers of a Gurugram-based clothing company h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cash-strapped holidaymakers are making their t...</td>\n",
       "      <td>Passengers are making their toddlers drag suit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>London, Jun 30 (PTI) Supermodel Gigi Hadid say...</td>\n",
       "      <td>Model Gigi Hadid has said she cannot pull off ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               ctext  \\\n",
       "0  India will host its first WTA tournament in fi...   \n",
       "1  Samajwadi Party vice-president Kiranmay Nanda,...   \n",
       "2  Gurgaon, Jul 7 (PTI) Gurgaon Police today regi...   \n",
       "3  Cash-strapped holidaymakers are making their t...   \n",
       "4  London, Jun 30 (PTI) Supermodel Gigi Hadid say...   \n",
       "\n",
       "                                                text  \n",
       "0  India will host its first WTA tournament in fi...  \n",
       "1  Ousted SP Vice President Kiranmoy Nanda, who w...  \n",
       "2  Servers of a Gurugram-based clothing company h...  \n",
       "3  Passengers are making their toddlers drag suit...  \n",
       "4  Model Gigi Hadid has said she cannot pull off ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset\n",
    "df = pd.read_parquet('../data/NewsSummary.parquet')\n",
    "# select the first 500 rows of the dataset\n",
    "df = df.iloc[:500, :]\n",
    "# select two columns\n",
    "df = df[['ctext', 'text']]   # original text and summary\n",
    "data = df.copy()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer done, with length 34994\n",
      "vocab size: 10000\n"
     ]
    }
   ],
   "source": [
    "TOKENIZER_MANAGER = TokenizerManager(num_words=10000)\n",
    "# TOKENIZER_MANAGER.train_tokenizer_from_csv(\"../data/bbc-news-summary.csv\")\n",
    "# TOKENIZER_MANAGER.save_tokenizer(\"../data/tokenizer_fixed.pkl\")\n",
    "\n",
    "TOKENIZER = TOKENIZER_MANAGER.load_tokenizer(load_path=\"../data/tokenizer_fixed.pkl\")\n",
    "print(\"tokenizer done, with length\", len(TOKENIZER.word_index) + 1)\n",
    "print(\"vocab size:\", TOKENIZER.num_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 34993\n",
      "Number of words: 10001\n",
      "<OOV>: 1\n",
      "the: 2\n",
      "to: 3\n",
      "of: 4\n",
      "a: 5\n",
      "and: 6\n",
      "in: 7\n",
      "is: 8\n",
      "for: 9\n",
      "that: 10\n",
      "said: 11\n",
      "it: 12\n",
      "on: 13\n",
      "was: 14\n",
      "he: 15\n",
      "be: 16\n",
      "has: 17\n",
      "with: 18\n",
      "p: 19\n",
      "have: 20\n",
      "as: 21\n",
      "at: 22\n",
      "will: 23\n",
      "by: 24\n",
      "not: 25\n",
      "are: 26\n",
      "but: 27\n",
      "i: 28\n",
      "from: 29\n",
      "mr: 30\n",
      "his: 31\n",
      "UNK_ID: 1\n",
      "sos token: 33\n",
      "eos token: 33\n"
     ]
    }
   ],
   "source": [
    "word2idx = TOKENIZER.word_index\n",
    "\n",
    "# show the length of the vocabulary\n",
    "print(f\"Vocabulary size: {len(word2idx)}\")\n",
    "print(f\"Number of words: {TOKENIZER.num_words + 1}\")\n",
    "\n",
    "idx2word = {v: k for k, v in word2idx.items()}\n",
    "\n",
    "for word, idx in word2idx.items():\n",
    "    print(f\"{word}: {idx}\")\n",
    "    if idx > 30:\n",
    "        break\n",
    "\n",
    "UNK_ID = word2idx['<OOV>']\n",
    "print(f\"UNK_ID: {UNK_ID}\")\n",
    "print(f\"sos token: {word2idx['sos']}\")\n",
    "print(f\"eos token: {TOKENIZER.word_index['sos']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['File_path', 'Articles', 'Summaries'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# load data for training and validation, size is LOADDATASIZE\n",
    "train_loader, val_loader = get_dataloaders_bbc(\"../data/bbc-news-summary.csv\", TOKENIZER, LOADDATASIZE, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model.GRU_seq2seq import GRUAttentionModel\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vocab_size = TOKENIZER.num_words + 1  # 词汇表大小\n",
    "embed_dim = 256     # embedding dimension for both encoder and decoder\n",
    "hidden_dim = 512        # hidden dimension for GRU\n",
    "num_heads = 8       # 多头注意力的头数\n",
    "dropout_rate = 0.3  # Dropout 概率\n",
    "\n",
    "model = GRUAttentionModel(vocab_size, embed_dim, hidden_dim, num_heads=num_heads, dropout_rate=dropout_rate)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0008)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output \n",
    "\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "  # use cross entropy loss for seq2seq model\n",
    "\n",
    "LOSS_THRESHOLD= 0.06  # loss threshold for early stopping\n",
    "EPOCH = 1000\n",
    "\n",
    "def train_gru_attention_model(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    optimizer, \n",
    "    loss_fn, \n",
    "    epochs=10, \n",
    "    checkpoint_path=\"gru_attention_checkpoint.h5\"\n",
    "):\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        train_loss = 0.0\n",
    "\n",
    "        # tqdm 训练进度条\n",
    "        train_progress = tqdm(train_loader, desc=f\"Training {epoch+1}/{epochs}\", unit=\"batch\")\n",
    "        for encoder_input, target_output in train_progress:\n",
    "            # 将 PyTorch tensor 转为 TensorFlow tensor\n",
    "            encoder_input = tf.convert_to_tensor(encoder_input.numpy(), dtype=tf.int32)\n",
    "            target_output = tf.convert_to_tensor(target_output.numpy(), dtype=tf.int32)\n",
    "            \n",
    "            # 构建解码器输入 (Teacher Forcing)\n",
    "            decoder_input = tf.concat(\n",
    "                [tf.fill([encoder_input.shape[0], 1], TOKENIZER.word_index['sos']), \n",
    "                 target_output[:, :-1]], \n",
    "                axis=-1\n",
    "            )\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(encoder_input, decoder_input)\n",
    "                loss = loss_fn(target_output, predictions)\n",
    "            \n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            train_loss += loss.numpy()\n",
    "            train_progress.set_postfix(loss=loss.numpy())\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_loss_history.append(train_loss)\n",
    "        # print(f\"\\nTrain Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # Validation Loss Calculation\n",
    "        val_loss = 0.0\n",
    "        val_progress = tqdm(val_loader, desc=\"Validation\", unit=\"batch\")\n",
    "        for encoder_input, target_output in val_progress:\n",
    "            encoder_input = tf.convert_to_tensor(encoder_input.numpy(), dtype=tf.int32)\n",
    "            target_output = tf.convert_to_tensor(target_output.numpy(), dtype=tf.int32)\n",
    "            \n",
    "            decoder_input = tf.concat(\n",
    "                [tf.fill([encoder_input.shape[0], 1], TOKENIZER.word_index['sos']), \n",
    "                 target_output[:, :-1]], \n",
    "                axis=-1\n",
    "            )\n",
    "            \n",
    "            predictions = model(encoder_input, decoder_input)\n",
    "            loss = loss_fn(target_output, predictions)\n",
    "            val_loss += loss.numpy()\n",
    "            val_progress.set_postfix(loss=loss.numpy())\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            clear_output(wait=True)\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_loss_history.append(val_loss)\n",
    "        print(f\"Epoch {epoch+1} - Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            model.save_weights(\"checkpoint/GRU_seq2seq_bbc_\"+str(LOADDATASIZE)+\"_fixed_bestStop.h5\")\n",
    "            print(\"✅ Model saved as the best checkpoint.\")\n",
    "\n",
    "        if train_loss < LOSS_THRESHOLD or val_loss < LOSS_THRESHOLD:\n",
    "            model.save_weights(checkpoint_path)\n",
    "            print(f'Early stop at epoch {epoch}: Loss is below the threshold {train_loss:.4f}.')\n",
    "            break\n",
    "    model.save_weights(checkpoint_path)\n",
    "    print(f'stop at epoch {epoch}: - Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "            \n",
    "\n",
    "train_gru_attention_model(\n",
    "\tmodel=model, \n",
    "\ttrain_loader=train_loader, \n",
    "\tval_loader=val_loader, \n",
    "\toptimizer=optimizer, \n",
    "\tloss_fn=loss_fn, \n",
    "\tepochs=EPOCH, \n",
    "\tcheckpoint_path=\"checkpoint/GRU_seq2seq_bbc_\"+str(LOADDATASIZE)+\"_fixed.h5\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ✅ 绘图：训练集 & 验证集 Loss 曲线\n",
    "print(\"==============================\")\n",
    "plt.plot(range(1, len(train_loss_history)+1), train_loss_history, label='Train Loss', marker='o')\n",
    "plt.plot(range(1, len(val_loss_history)+1), val_loss_history, label='Val Loss', marker='x')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"checkpoint/loss_curve_GRU_bbc_\"+str(LOADDATASIZE)+\"_fixed.png\")\n",
    "# plt.show()\n",
    "print(\"✅ Loss curve diagram is saved successfully\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN3w/+08aLgrnu3a9KJo653",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
